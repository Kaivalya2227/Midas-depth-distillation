
# Predicting relative depth from a single image by training a CNN with self-supervised losses

This project explores the feasibility of learning relative depth from a single image using deep learning.
We implement a teacher-student framework where a lightweight U-Net (student) is trained on pseudo-depth labels generated from a pre-trained MiDaS model (teacher), using augmented views of the provided image.

---

##  Project Structure

```
u7839703.zip/MidTermProject
│
├── u7839703_MidTermProject_Code.ipynb          # Core code with training, evaluation, and visualization
├── jcsmr.jpg             # Provided input image (John Curtin School of Medical Research)
├── u7839703.pdf          # Report
└── README.md             # This file
```

---

##  Task Overview

- **Objective**: Predict relative depth from a single RGB image.
- **Dataset**: Single provided image with random augmentations.
- **Supervision**: Pseudo-labels of augmentations, generated by MiDaS (pre-trained teacher).
- **Model**: Student-teacher U-Net-like CNN trained on augmented crops.
- **Losses**:
  - Multi-scale MSE loss
  - Edge-aware smoothness loss

---

##  Setup and Dependencies

Tested on **Google Colab** with Python 3.10 and CUDA.

###  Install Required Packages

```bash
pip install torch torchvision timm opencv-python matplotlib
```

---

##  How to Test the Code

### 1. Upload `jcsmr.jpg` to the working directory.

### 2. Open and execute `code.ipynb`.

This notebook will:
- Load the teacher model (MiDaS).
- Augment the base image into a pseudo-dataset.
- Train the student model.
- Visualize predictions.
- Compute the evaluation metrics, RMSE, MAE, and δ-accuracy thresholds.

---

##  Output

The notebook plots the predicted depth maps:

| Teacher (MiDaS) Depth   | Student Predicted Depth |
|-------------------------|-----------------------------|
| ![](sample_teacher.png) | ![](sample_student.png)     |

---
### By:
Kaivalya Karanam (u7839703)
