{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3ZB_iog7g6i8"
      },
      "outputs": [],
      "source": [
        "#Reference of pre-trained model being used (MiDaS) - https://github.com/isl-org/MiDaS\n",
        "\n",
        "# Installing pytorch, timm for MiDas model, opencv, matplotlib for graphs\n",
        "!pip install -q torch torchvision timm opencv-python matplotlib\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "\n",
        "from torch.utils.data import Dataset, DataLoader, random_split\n",
        "from torchvision import transforms as T\n",
        "from PIL import Image\n",
        "\n",
        "import numpy as np\n",
        "import cv2\n",
        "import matplotlib.pyplot as plt\n",
        "import random                                     # Introduce a random seed for consistency\n",
        "torch.manual_seed(42)\n",
        "np.random.seed(42)\n",
        "random.seed(42)\n",
        "\n",
        "# Using GPU if available\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(\"Using device:\", device)\n",
        "\n",
        "# Loading a MiDas_small model, transforms for pseudo labels in training the model with augumentations\n",
        "midas = torch.hub.load(\"intel-isl/MiDaS\", \"MiDaS_small\").to(device).eval()\n",
        "tfm   = torch.hub.load(\"intel-isl/MiDaS\", \"transforms\").small_transform\n",
        "\n",
        "# Creating a dataset with different augumentations of the given image.\n",
        "# Length of dataset = 300 and size of each image in dataset is 256*256.\n",
        "class PseudoDepthDataset(Dataset):\n",
        "    def __init__(self, pil_img, length=300, target_size=(256, 256)):\n",
        "        self.base = pil_img\n",
        "        self.length = length\n",
        "        self.target_sz = target_size\n",
        "\n",
        "        self.aug = T.Compose([                         # Added parameter p for randomness\n",
        "            T.RandomHorizontalFlip(p=0.5),\n",
        "            T.RandomVerticalFlip(p=0.1),\n",
        "            T.RandomRotation(10, fill=(0, 0, 0)),\n",
        "            T.ColorJitter(0.3, 0.3, 0.2, 0.2),\n",
        "            T.Resize(self.target_sz, interpolation=Image.BILINEAR)\n",
        "        ])\n",
        "        self.to_tensor = T.ToTensor()\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.length\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        aug_pil = self.aug(self.base)\n",
        "        img_t = self.to_tensor(aug_pil)\n",
        "\n",
        "        np_rgb = np.array(aug_pil)\n",
        "        np_bgr = cv2.cvtColor(np_rgb, cv2.COLOR_RGB2BGR)\n",
        "\n",
        "        batch = tfm(np_bgr).to(device)\n",
        "        with torch.no_grad():\n",
        "            depth = midas(batch)\n",
        "\n",
        "        depth = depth.squeeze(1).squeeze(0).cpu()\n",
        "        return img_t, depth.unsqueeze(0)           # Input image (Tensor), Target Depth map from MiDas (using this as label)\n",
        "\n",
        "# Loading input image and creating a dataset with 300 augmentations.\n",
        "base_img = Image.open(\"/content/jcsmr.jpg\").convert(\"RGB\")\n",
        "dataset = PseudoDepthDataset(base_img, length=300)\n",
        "\n",
        "# Dividing the dataset into training, validation sets in ration of 80:20.\n",
        "train_len = int(0.8 * len(dataset))\n",
        "val_len = len(dataset) - train_len\n",
        "train_ds, val_ds = random_split(dataset, [train_len, val_len])\n",
        "\n",
        "train_dl = DataLoader(train_ds, batch_size=8, shuffle=True, num_workers=0, pin_memory=True)\n",
        "val_dl = DataLoader(val_ds, batch_size=8, shuffle=False, num_workers=0, pin_memory=True)\n",
        "\n",
        "# Creating a student U-Net architecture\n",
        "class ConvBlock(nn.Module):\n",
        "    def __init__(self, in_c, out_c):\n",
        "        super().__init__()\n",
        "        self.block = nn.Sequential(\n",
        "            nn.Conv2d(in_c, out_c, 3, padding=1),\n",
        "            nn.BatchNorm2d(out_c),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(out_c, out_c, 3, padding=1),\n",
        "            nn.BatchNorm2d(out_c),\n",
        "            nn.ReLU()\n",
        "        )\n",
        "    def forward(self, x):\n",
        "        return self.block(x)\n",
        "\n",
        "# Introducing encoders (to compress input), decoders (upsample back to depth map size)\n",
        "class StudentNet(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.enc1 = ConvBlock(3, 32)     # 2 convolution layers\n",
        "        self.enc2 = nn.Sequential(nn.MaxPool2d(2), ConvBlock(32, 64))  # 2 convolution layers\n",
        "        self.dec1 = nn.Sequential(nn.ConvTranspose2d(64, 32, 2, stride=2), nn.ReLU())  # 1 deconvolution layer\n",
        "        self.out  = nn.Conv2d(32, 1, 3, padding=1)  # 1 convolution layer\n",
        "\n",
        "    def forward(self, x):\n",
        "        x1 = self.enc1(x)       # First encoder block\n",
        "        x2 = self.enc2(x1)      # Second encoder block, with downsampling\n",
        "        y  = self.dec1(x2)      # Decoder\n",
        "        y  = y + x1             # Skip connection: add encoder output to decoder\n",
        "        return self.out(y)      # Final output convolution layer, 1-channel depth output\n",
        "\n",
        "# Computing MSE, edge_smooth loss functions for training\n",
        "# Calculates MSE loss at full, half, quarter size resolutions.\n",
        "def multiscale_loss(pred, target, base_loss):\n",
        "    loss = base_loss(pred, target)\n",
        "    for scale in [2, 4]:\n",
        "        p = F.interpolate(pred, scale_factor=1/scale, mode='bilinear', align_corners=False)\n",
        "        t = F.interpolate(target, scale_factor=1/scale, mode='bilinear', align_corners=False)\n",
        "        loss += base_loss(p, t)\n",
        "    return loss\n",
        "\n",
        "# edge-aware smoothness loss to enforce spatial consistency, preserves edges.\n",
        "def edge_smooth(pred, img):\n",
        "    dy   = torch.abs(pred[:, :, 1:, :] - pred[:, :, :-1, :])\n",
        "    dx   = torch.abs(pred[:, :, :, 1:] - pred[:, :, :, :-1])\n",
        "    wgty = torch.exp(-torch.mean(torch.abs(img[:, :, 1:, :] - img[:, :, :-1, :]), 1, keepdim=True))\n",
        "    wgtx = torch.exp(-torch.mean(torch.abs(img[:, :, :, 1:] - img[:, :, :, :-1]), 1, keepdim=True))\n",
        "    return (dx * wgtx).mean() + (dy * wgty).mean()\n",
        "\n",
        "# Training the model with training, validation sets as per the epochs\n",
        "student = StudentNet().to(device)\n",
        "opt = optim.Adam(student.parameters(), lr=1e-4)    # Learning rate, using adam optimiser to update weights\n",
        "mse = nn.MSELoss()\n",
        "epochs = 7\n",
        "\n",
        "for ep in range(epochs):\n",
        "    student.train()\n",
        "    train_loss = 0.0\n",
        "    for imgs, depths in train_dl:\n",
        "        imgs, depths = imgs.to(device), depths.to(device)\n",
        "        preds = student(imgs)\n",
        "        loss = multiscale_loss(preds, depths, mse) + edge_smooth(preds, imgs) * 0.1     # Using both MSE loss and edge_smooth loss\n",
        "\n",
        "        opt.zero_grad()    # Resets the gradients of all model parameters to zero.\n",
        "        loss.backward()    # Computes the gradients of the loss function with respect to each model parameter\n",
        "        opt.step()         # Updates weights\n",
        "        train_loss += loss.item()\n",
        "\n",
        "    student.eval()\n",
        "    val_loss = 0.0\n",
        "    with torch.no_grad():\n",
        "        for imgs, depths in val_dl:\n",
        "            imgs, depths = imgs.to(device), depths.to(device)\n",
        "            preds = student(imgs)\n",
        "            loss = multiscale_loss(preds, depths, mse) + edge_smooth(preds, imgs) * 0.1\n",
        "            val_loss += loss.item()\n",
        "\n",
        "    print(f\"Epoch {ep+1}/{epochs} | Train Loss: {train_loss/len(train_dl):.4f} | Val Loss: {val_loss/len(val_dl):.4f}\")\n",
        "\n",
        "# TTA (Test-Time Augmentation)\n",
        "# Flips the image horizontally, predicts again and averages both predictions.\n",
        "# To reduce prediction noise and improve generalization.\n",
        "def predict_with_tta(model, img):\n",
        "    p1 = model(img)\n",
        "    img_flipped = torch.flip(img, dims=[-1])\n",
        "    p2 = model(img_flipped)\n",
        "    p2 = torch.flip(p2, dims=[-1])\n",
        "    return (p1 + p2) / 2\n",
        "\n",
        "student.eval()\n",
        "with torch.no_grad():\n",
        "    resized_base = base_img.resize((256, 256))\n",
        "    inp = T.ToTensor()(resized_base).unsqueeze(0).to(device)\n",
        "\n",
        "    # Student prediction\n",
        "    outd = predict_with_tta(student, inp)[0, 0].cpu().numpy()\n",
        "\n",
        "    # Teacher prediction\n",
        "    np_bgr = cv2.cvtColor(np.array(resized_base), cv2.COLOR_RGB2BGR)\n",
        "    teacher_depth = midas(tfm(np_bgr).to(device)).squeeze().cpu().numpy()\n",
        "\n",
        "# Comparing both teacher and student model outputs\n",
        "plt.figure(figsize=(10, 5))\n",
        "\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.imshow(teacher_depth, cmap=\"plasma\")\n",
        "plt.title(\"Teacher (MiDaS) Depth\")\n",
        "plt.axis(\"off\")\n",
        "\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.imshow(outd, cmap=\"plasma\")\n",
        "plt.title(\"Student Predicted Depth\")\n",
        "plt.axis(\"off\")\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Computing metrics RMSE (Root Mean Square Error), MAE (Mean Absolute Error), delta\n",
        "# RMSE = sqrt(mean((pred - target)^2))\n",
        "# MAE = mean(abs(pred - target))\n",
        "# δ accuracy: % of pixels where prediction is within a factor t of the teacher depth\n",
        "# δ = max(pred / target, target / pred)\n",
        "# delta_acc = mean(delta < threshold) for t in [1.25, 1.25^2, 1.25^3]\n",
        "def compute_metrics(pred, target):\n",
        "    pred = pred.flatten()\n",
        "    target = target.flatten()\n",
        "\n",
        "    pred = np.clip(pred, 1e-6, None)\n",
        "    target = np.clip(target, 1e-6, None)\n",
        "\n",
        "    abs_diff = np.abs(pred - target)\n",
        "    sq_diff = (pred - target) ** 2\n",
        "\n",
        "    rmse = np.sqrt(np.mean(sq_diff))\n",
        "    mae = np.mean(abs_diff)\n",
        "\n",
        "    ratio = np.maximum(pred / target, target / pred)\n",
        "    delta1 = np.mean(ratio < 1.25)\n",
        "    delta2 = np.mean(ratio < 1.25 ** 2)\n",
        "    delta3 = np.mean(ratio < 1.25 ** 3)\n",
        "\n",
        "    return {\n",
        "        \"RMSE\": rmse,\n",
        "        \"MAE\": mae,\n",
        "        \"δ < 1.25\": delta1,\n",
        "        \"δ < 1.25²\": delta2,\n",
        "        \"δ < 1.25³\": delta3\n",
        "    }\n",
        "\n",
        "metrics = compute_metrics(outd, teacher_depth)\n",
        "print(\"\\n--- Evaluation Metrics ---\")\n",
        "for k, v in metrics.items():\n",
        "    print(f\"{k}: {v:.4f}\")"
      ]
    }
  ]
}